# NLP 
```
a task => LLM -
- text classification
- Tráº£ lá»i cÃ¢u há»i (question answering)
- PhÃ¢n loáº¡i khÃ´ng cáº§n huáº¥n luyá»‡n trÆ°á»›c (zero-shot classification)
- Dá»‹ch thuáº­t (translation)
- TÃ³m táº¯t vÄƒn báº£n (text summarization)
- feature extraction nhÆ° nháº­n dáº¡ng thá»±c thá»ƒ (NER), ...
- text generation
- Äiá»n tá»« thiáº¿u (fill-mask)
- sentence similarity -> result
```

`User prompt` - security -> `LLM` - safty -> `LLM response`
* `Security` Ä‘Ã£m báº£o nhá»¯ng ai cÃ³ tháº©m quyá»n má»›i má»Ÿi cÃ³ data tÆ°Æ¡ng á»©ng, prompt Ä‘Ã¡ng tin cáº­y má»›i Ä‘Æ°á»£c xá»­ lÃ½
* `Safty` Ä‘Ã£m báº£o AI hoáº¡t Ä‘á»™ng cÃ³ trÃ¡ch nhiá»‡m vÃ  Ä‘áº¡o Ä‘á»©c, táº­p trung vÃ o viá»‡c giáº£m thiá»ƒu ná»™i dung Ä‘á»™c háº¡i Ä‘áº¿n user hoáº·c há»‡ thá»‘ng káº¿t ná»‘i khÃ¡c nhÆ° Biases, Hallucinations, leaks Personally identifiable information (PII), ...


#### AutoIntent 
* is an open source tool for automatic configuration of text classification pipelines, with specialized support for intent prediction.
* https://deeppavlov.github.io/AutoIntent/versions/dev/index.html#getting-started
* tÆ°Æ¡ng tÃ¡c vá»›i LLM Ä‘á»ƒ tÄƒng cÆ°á»ng dá»¯ liá»‡u vá»›i dspy: https://deeppavlov.github.io/AutoIntent/versions/dev/augmentation_tutorials/dspy_augmentation.html

#### ğŸ§  Kiáº¿n trÃºc káº¿t há»£p Rasa + RAG

1. **Hiá»ƒu Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng (intent)**
2. **TrÃ­ch xuáº¥t thá»±c thá»ƒ (entity)**
3. **Sinh metadata bá»• sung (vÃ­ dá»¥: loáº¡i cÃ¢u há»i, chá»§ Ä‘á», má»©c Ä‘á»™ Æ°u tiÃªn, v.v.)**

â†’ Sau Ä‘Ã³ dÃ¹ng **RAG (Retrieval-Augmented Generation)** hoáº·c má»™t **LLM agent** Ä‘á»ƒ truy xuáº¥t thÃ´ng tin vÃ  táº¡o cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n.

```text
User input â†’ Rasa NLU â†’ { intent, entities, metadata }
                            â†“
                    Custom logic / bridge
                            â†“
       â†’ RAG engine (FAISS / Weaviate / Elasticsearch + LLM)
                            â†“
                 Final answer to user
```

âœ”ï¸ **Rasa ráº¥t phÃ¹ há»£p Ä‘á»ƒ lÃ m lá»›p NLP tiá»n xá»­ lÃ½ trÆ°á»›c RAG**, nhá» vÃ o kháº£ nÄƒng:
* TrÃ­ch xuáº¥t ngá»¯ nghÄ©a cÃ³ cáº¥u trÃºc (intent, entity)
* Bá»• sung metadata tÃ¹y chá»‰nh
* Káº¿t ná»‘i Ä‘Æ°á»£c vá»›i downstream RAG pipeline qua REST API, Python service hoáº·c custom action

ğŸ“¦ VÃ­ dá»¥ cá»¥ thá»ƒ: ask: â€œLÃ m sao Ä‘á»ƒ cáº­p nháº­t pháº§n má»m trÃªn thiáº¿t bá»‹ X?â€
1. **Rasa NLU xá»­ lÃ½ Ä‘áº§u vÃ o**:
```json
{
  "intent": { "name": "ask_software_update", "confidence": 0.97 },
  "entities": [
    { "entity": "device", "value": "thiáº¿t bá»‹ X" }
  ],
  "text": "LÃ m sao Ä‘á»ƒ cáº­p nháº­t pháº§n má»m trÃªn thiáº¿t bá»‹ X?",
  "metadata": {
    "language": "vi",
    "question_type": "how_to"
  }
}
```

2. **Báº¡n dÃ¹ng intent + entities + metadata lÃ m Ä‘iá»u kiá»‡n truy váº¥n vÃ o RAG**:

```python
query = f"Cáº­p nháº­t pháº§n má»m cho {entity['device']}"
retrieved_docs = retriever.search(query)
answer = llm.generate_answer(query, context=retrieved_docs)
```

ğŸ§ª Gá»£i Ã½ táº¯ng cÆ°á»ng metadata
* Gáº¯n thÃªm **intent whitelist**: chá»‰ forward cÃ¡c intent Ä‘Æ°á»£c phÃ©p truy vÃ o RAG
* Gáº¯n **retrieval filters theo entity**: vÃ­ dá»¥: náº¿u entity lÃ  `product: X`, chá»‰ truy trong knowledge base vá» X
* ThÃªm **metadata enrichment**: vÃ­ dá»¥: xÃ¡c Ä‘á»‹nh `tone`, `sentiment`, `urgency`, `topic`, `department`, v.v.

TÃ­nh Ä‘á»™ liÃªn quan Ä‘áº¿n topic
```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("all-MiniLM-L6-v2")
chunk_embedding = model.encode(chunk_text)

# topic gá»“m ["oncology", "neurology", "finance", "law"] â†’ score tá»«ng label
topic_embedding = model.encode("finance")

score = util.cos_sim(chunk_embedding, topic_embedding)
```

TÃ­nh Ä‘á»™ "vÃ´ dá»¥ng" vá» máº·t semantic cho topic Ä‘Ã³ (functional noise)
```python
def compute_topic_noise(chunk_text, topic, topic_embedding_dict):
    # 1. Semantic similarity (relevance)
    chunk_embedding = embed(chunk_text)
    relevance = cosine_similarity(chunk_embedding, topic_embedding_dict[topic])
    
    # 2. Heuristic noise detection (optional)
    has_html = detect_html_noise(chunk_text)
    is_too_short = len(chunk_text.split()) < 10
    filler_words = detect_filler_content(chunk_text)
    
    technical_noise = 0.0
    if has_html or is_too_short or filler_words:
        technical_noise = 0.6  # or a heuristic value
    
    # 3. Semantic noise = 1 - relevance
    semantic_noise = 1.0 - relevance
    
    # 4. Final noise = weighted combination
    final_noise = 0.7 * semantic_noise + 0.3 * technical_noise
    return final_noise
```

---
### **NLP giÃºp cáº¥u trÃºc láº¡i prompt** => **LLM dá»… hiá»ƒu vÃ  tráº£ lá»i chÃ­nh xÃ¡c**

> NLP khÃ´ng chá»‰ giÃºp *tÄƒng Ä‘á»™ chÃ­nh xÃ¡c* mÃ  cÃ²n *tá»‘i Æ°u hÃ³a chi phÃ­* vÃ  *nÃ¢ng cao tÃ­nh tin cáº­y* trong há»‡ thá»‘ng RAG

* TÄƒng Ä‘á»™ chÃ­nh xÃ¡c
* Giáº£m hiá»‡n tÆ°á»£ng "hallucination"
* Tá»‘i Æ°u hÃ³a Ä‘á»™ dÃ i prompt (token efficiency)
* VÃ  tÄƒng kháº£ nÄƒng reasoning cá»§a LLM

```
query â†’ retriever â†’ re_ranker â†’ summarizer â†’ structured_prompt â†’ LLM
```

| BÆ°á»›c NLP                    | Má»¥c tiÃªu                    | CÃ´ng cá»¥ gá»£i Ã½              |
| --------------------------- | --------------------------- | -------------------------- |
| Chunk re-ranking            | Giá»¯ ná»™i dung liÃªn quan nháº¥t | BM25, Cohere ReRank        |
| Query-focused summarization | TÃ³m táº¯t sÃ¡t vá»›i truy váº¥n    | LLM, LangChain, LlamaIndex |
| Coreference resolution      | Giáº£m mÆ¡ há»“ trong ngÃ´n ngá»¯   | SpaCy, coreferee           |
| Prompt structuring          | TÄƒng kháº£ nÄƒng reasoning     | Prompt templates           |
| Paraphrasing                | RÃºt gá»n nhÆ°ng giá»¯ nghÄ©a     | Pegasus, T5                |
| Entity grounding            | TÄƒng Ä‘á»™ tin cáº­y             | Citation tagging           |
| Semantic grouping           | Gom ná»™i dung theo chá»§ Ä‘á»    | Topic modeling, NER        |

Viá»‡c gom nhÃ³m (clustering) tÃ i liá»‡u + tÃ³m táº¯t theo cá»¥m (chunk summarization) + sáº¯p xáº¿p cÃ³ logic (thematic ordering):

* â¡ï¸ TÄƒng kháº£ nÄƒng hiá»ƒu ná»™i dung vÃ  suy luáº­n theo máº¡ch cho LLM vÃ¬ `khÃ´ng cáº§n â€œxÃ¢u chuá»—iâ€ cÃ¡c Ä‘oáº¡n rá»i ráº¡c`
* â¡ï¸ Giáº£m Ä‘á»™ phÃ¢n máº£nh ngá»¯ nghÄ©a cá»§a prompt vÃ¬ `prompt Ä‘Ã£ cÃ³ trÃ¬nh tá»±, dá»… xá»­ lÃ½ hÆ¡n vá» máº·t reasoning` => `TÄƒng trá»ng sá»‘ ngá»¯ nghÄ©a`
* â¡ï¸ Táº­p trung LLM vÃ o cÃ¡c chá»§ Ä‘á» cá»‘t lÃµi, thay vÃ¬ xá»­ lÃ½ 10 Ä‘oáº¡n rá»i ráº¡c khÃ´ng liÃªn quan

**Táº¡o "ngá»¯ cáº£nh Ä‘a táº§ng" (hierarchical context)** vÃ¬ LLM xá»­ lÃ½ theo táº§ng:

* TÃ³m táº¯t nhÃ³m â†’ hiá»ƒu chá»§ Ä‘á» chung
* Chi tiáº¿t trong nhÃ³m â†’ tÃ¬m thÃ´ng tin há»— trá»£
* Tá»•ng há»£p theo truy váº¥n

```text
Context (Semantic Clusters):
----------------------------
[Cluster 1: Vá» Ä‘iá»u kiá»‡n phÃ¡p lÃ½]
- TÃ³m táº¯t: NgÆ°á»i ná»™p Ä‘Æ¡n cáº§n cÃ³ há»£p Ä‘á»“ng lao Ä‘á»™ng há»£p phÃ¡p vÃ  Ä‘á»§ Ä‘iá»u kiá»‡n lÆ°u trÃº.
- Chi tiáº¿t:
  + Chunk 1.1
  + Chunk 1.2

[Cluster 2: Vá» tÃ i chÃ­nh]
- TÃ³m táº¯t: á»¨ng viÃªn cáº§n chá»©ng minh Ä‘á»§ kháº£ nÄƒng tÃ i chÃ­nh trong 6 thÃ¡ng gáº§n nháº¥t.
- Chi tiáº¿t:
  + Chunk 2.1
  + Chunk 2.2

...

Instruction:
- Dá»±a trÃªn cÃ¡c thÃ´ng tin trong cÃ¡c cá»¥m trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i sau:
Q: Nhá»¯ng yÃªu cáº§u quan trá»ng nháº¥t Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p visa lÃ  gÃ¬?
```

#### 1. **Chunk Ranking / Filtering**

ğŸ‘‰ **Má»¥c tiÃªu**: Giá»¯ láº¡i cÃ¡c Ä‘oáº¡n liÃªn quan nháº¥t, loáº¡i bá» nhiá»…u.

* Sá»­ dá»¥ng: **semantic similarity**, **keyword overlap**, **query-focused summarization**.
* CÃ´ng cá»¥: `sentence-transformers`, `BM25`, `re-rankers` nhÆ° Cohere ReRank, OpenAI re-ranking APIs.
* GiÃºp LLM **chá»‰ Ä‘á»c thÃ´ng tin cáº§n thiáº¿t**, trÃ¡nh bá»‹ "quÃ¡ táº£i thÃ´ng tin".

#### 2. **Query-aware Summarization**

ğŸ‘‰ **Má»¥c tiÃªu**: TÃ³m táº¯t cÃ¡c chunks theo **truy váº¥n cá»§a ngÆ°á»i dÃ¹ng**, chá»© khÃ´ng tÃ³m táº¯t chung chung.

* VÃ­ dá»¥: Tá»« 3 Ä‘oáº¡n tÃ i liá»‡u, táº¡o 1 Ä‘oáº¡n tÃ³m gá»n Ä‘Ãºng ngá»¯ cáº£nh cÃ¢u há»i.
* GiÃºp prompt gá»n hÆ¡n mÃ  váº«n Ä‘áº§y Ä‘á»§ Ã½ cáº§n thiáº¿t.

ğŸ”§ Tools: LLM-based summarizers (`map-reduce`, `refine`, hoáº·c `LLM summarization chains` trong LangChain, LlamaIndex...).

#### 3. **Coreference Resolution & Named Entity Normalization**

ğŸ‘‰ **Má»¥c tiÃªu**: Xá»­ lÃ½ Ä‘áº¡i tá»« mÆ¡ há»“ vÃ  thá»‘ng nháº¥t thá»±c thá»ƒ.

* VÃ­ dá»¥:

  > "Ã”ng áº¥y nÃ³i sáº½ ná»™p Ä‘Æ¡n."
  > â†’ "Ã”ng **Nguyá»…n VÄƒn A** nÃ³i sáº½ ná»™p Ä‘Æ¡n."

* Äiá»u nÃ y giÃºp LLM **khÃ´ng bá»‹ hiá»ƒu nháº§m** khi xá»­ lÃ½ chuá»—i chunks cÃ³ nhiá»u thá»±c thá»ƒ.

ğŸ”§ Tools: SpaCy, AllenNLP, Hugging Face `coreferee`, etc.

#### 4. **Prompt Template Structuring (Cáº¥u trÃºc hÃ³a Prompt)**

ğŸ‘‰ **Má»¥c tiÃªu**: ÄÆ°a knowledge vÃ o cáº¥u trÃºc rÃµ rÃ ng, giÃºp LLM suy luáº­n tá»‘t hÆ¡n

```
Relevant Facts:
1. <fact from chunk1>
2. <fact from chunk2>
3. <fact from chunk3>

Instruction: Dá»±a trÃªn cÃ¡c thÃ´ng tin trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i sau:
Q: TÃ³m táº¯t ná»™i dung chÃ­nh?
```

â†’ LLM xá»­ lÃ½ tá»‘t hÆ¡n vÃ¬ thÃ´ng tin **cÃ³ thá»© tá»±, rÃµ ngá»¯ nghÄ©a**, dá»… xá»­ lÃ½ logic

#### 5. **Chunk Paraphrasing or Compression**

ğŸ‘‰ **Má»¥c tiÃªu**: NÃ©n chunk theo nghÄ©a, khÃ´ng máº¥t thÃ´ng tin quan trá»ng.

* DÃ¹ng paraphrasing Ä‘á»ƒ diá»…n Ä‘áº¡t láº¡i ná»™i dung phá»©c táº¡p thÃ nh cÃ¢u ngáº¯n, dá»… hiá»ƒu.
* Giáº£m sá»‘ token, tÄƒng Ä‘á»™ hiá»‡u quáº£.

#### 6. **Entity Grounding / Context Linking**

ğŸ‘‰ **Má»¥c tiÃªu**: RÃ ng buá»™c thá»±c thá»ƒ vá»›i nguá»“n rÃµ rÃ ng Ä‘á»ƒ giáº£m â€œáº£o giÃ¡câ€.

* ThÃªm trÃ­ch dáº«n inline, vÃ­ dá»¥:

  > "\[1] Tá»• chá»©c XYZ quy Ä‘á»‹nh ráº±ng..."
  > Hoáº·c: "(Nguá»“n: Luáº­t ABC, Äiá»u 5)"

* GiÃºp LLM pháº£n há»“i cÃ³ cÄƒn cá»©, tÄƒng Ä‘á»™ tin cáº­y Ä‘áº§u ra.

#### 7. **Semantic Segmentation + Prompt Merging**

ğŸ‘‰ **Má»¥c tiÃªu**: Gom cÃ¡c Ä‘oáº¡n cÃ³ Ã½ nghÄ©a liÃªn káº¿t thÃ nh 1 pháº§n trong prompt

VÃ­ dá»¥: Náº¿u 2 chunks cÃ¹ng nÃ³i vá» â€œÄ‘iá»u kiá»‡n xin visaâ€, thÃ¬ gá»™p chÃºng vÃ  phÃ¢n loáº¡i theo chá»§ Ä‘á»

â†’ Káº¿t quáº£ prompt dá»… xá»­ lÃ½ hÆ¡n vÃ¬ Ã­t "nháº£y chá»§ Ä‘á»".

---
### DÃ¹ng NLP Ä‘á»ƒ cáº£i thiá»‡n RAG bá»Ÿi **Metadata Enhancement** vÃ  **Context Enrichment**

* DÃ¹ng thÆ° viá»‡n nhÆ° **spaCy, spacy-llm** cho cÃ¡c task NLP
* DÃ¹ng NLP Ä‘á»ƒ **tiá»n xá»­ lÃ½ (preprocessing)** chunks
* LÆ°u metadata & enriched version cá»§a má»—i chunk song song (hoáº·c lÃ m multi-field trong embedding)


#### ğŸ§© 1. NLP há»— trá»£ **Metadata Enhancement**

ğŸ¯ Má»¥c tiÃªu: TÄƒng cÆ°á»ng thÃ´ng tin mÃ´ táº£ cho má»—i chunk vÄƒn báº£n

| NLP Task                               | Má»¥c Ä‘Ã­ch                           | VÃ­ dá»¥                                |
| -------------------------------------- | ---------------------------------- | ------------------------------------ |
| ğŸ§¾ **Named Entity Recognition (NER)**  | TrÃ­ch xuáº¥t thá»±c thá»ƒ                | NgÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, cÃ´ng ty, bá»‡nh lÃ½... |
| ğŸ§  **Topic Classification**            | XÃ¡c Ä‘á»‹nh chá»§ Ä‘á» chunk              | `finance`, `legal`, `healthcare`...  |
| ğŸ“… **Date Extraction & Normalization** | Gáº¯n thá»i gian, chuáº©n hÃ³a Ä‘á»‹nh dáº¡ng | â€œThÃ¡ng 9 nÄƒm ngoÃ¡iâ€ â†’ `2024-09`      |
| ğŸ“š **Document Classification**         | GÃ¡n loáº¡i tÃ i liá»‡u                  | Há»£p Ä‘á»“ng, bÃ¡o cÃ¡o, email...          |
| ğŸ‘¤ **Author Attribution (náº¿u cÃ³)**     | GÃ¡n tÃ¡c giáº£ dá»±a trÃªn vÄƒn phong     | PhÃ¢n biá»‡t cÃ¡c ngÆ°á»i viáº¿t             |
| ğŸ“ **Readability Assessment**          | TÃ­nh Ä‘iá»ƒm khÃ³â€“dá»… Ä‘á»c               | DÃ¹ng Ä‘á»ƒ chá»n chunk phÃ¹ há»£p vá»›i user  |

```json
{
  "text": "...",
  "metadata": {
    "topic": "oncology",
    "entities": ["melanoma", "BRAF mutation"],
    "created_at": "2023-06-12",
    "readability_score": 61.2
  }
}
```

ğŸ‘‰ GiÃºp há»‡ thá»‘ng **filter, rerank, hoáº·c boost** káº¿t quáº£ tá»‘t hÆ¡n khi truy xuáº¥t

#### ğŸ§© 2. NLP há»— trá»£ **Context Enrichment**

ğŸ¯ Má»¥c tiÃªu: LÃ m giÃ u ná»™i dung vÄƒn báº£n Ä‘á»ƒ LLM dá»… hiá»ƒu vÃ  tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n

| NLP Task                              | Má»¥c Ä‘Ã­ch                                   | VÃ­ dá»¥                                            |
| ------------------------------------- | ------------------------------------------ | ------------------------------------------------ |
| ğŸ“š **Abbreviation Expansion**         | Má»Ÿ rá»™ng tá»« viáº¿t táº¯t                        | â€œCOPDâ€ â†’ â€œChronic Obstructive Pulmonary Diseaseâ€ |
| ğŸ”— **Coreference Resolution**         | Thay Ä‘áº¡i tá»« báº±ng thá»±c thá»ƒ                  | â€œÃ´ng áº¥yâ€ â†’ â€œbÃ¡c sÄ© TrÃ­â€                          |
| ğŸ§  **Concept Linking (Wikification)** | Gáº¯n link hoáº·c Ä‘á»‹nh nghÄ©a                   | â€œquantum tunnelingâ€ â†’ link Ä‘áº¿n Wikipedia         |
| ğŸ“– **Summarization**                  | Táº¡o báº£n tÃ³m táº¯t chunk                      | GiÃºp LLM xá»­ lÃ½ nhanh hÆ¡n                         |
| ğŸ”„ **Paraphrasing**                   | Viáº¿t láº¡i chunk theo phong cÃ¡ch dá»… hiá»ƒu hÆ¡n | Há»¯u Ã­ch vá»›i tÃ i liá»‡u khÃ³ Ä‘á»c                     |
| âš™ **Information Extraction (IE)**     | TÃ¡ch insight tá»« chunk                      | VÃ­ dá»¥: thuá»‘c Ä‘iá»u trá»‹, tÃ¡c dá»¥ng phá»¥, chá»‰ Ä‘á»‹nh... |
| ğŸ§¾ **Relation Extraction**            | Ná»‘i cÃ¡c khÃ¡i niá»‡m cÃ³ quan há»‡               | Bá»‡nh â†’ Triá»‡u chá»©ng â†’ Äiá»u trá»‹                    |

> âœ… **Káº¿t quáº£**: CÃ¡c chunk Ä‘Æ°á»£c lÃ m giÃ u Ä‘á»ƒ **giao tiáº¿p tá»‘t hÆ¡n vá»›i LLM**:

```text
Original:
"The patient has COPD. He was treated with a beta-agonist."

Enriched:
"The patient has Chronic Obstructive Pulmonary Disease (COPD). The patient was treated with a beta-agonist, a type of medication used to relax muscles of the airways."
```

ğŸ‘‰ LLM **hiá»ƒu rÃµ hÆ¡n**, sinh ná»™i dung **máº¡ch láº¡c vÃ  Ä‘Ãºng Ä‘Ã­ch hÆ¡n**, Ä‘áº·c biá»‡t khi dÃ¹ng trong QA hoáº·c tÃ³m táº¯t.


